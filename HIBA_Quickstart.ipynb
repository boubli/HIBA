{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå∏ HIBA-7B-Soul: Official Quickstart\n",
                "\n",
                "<div align=\"center\">\n",
                "  <img src=\"https://raw.githubusercontent.com/boubli/HIBA/main/docs/assets/logo.png\" width=\"120\" alt=\"HIBA Logo\"/>\n",
                "  <h2>HIBA: The Therapeutic AI Companion</h2>\n",
                "  <p><em>\"A Gift from God\" ‚Äî Built with love in Morocco üá≤üá¶</em></p>\n",
                "  <p><strong>100% Private ‚Ä¢ Free Forever ‚Ä¢ No API Key Needed</strong></p>\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "### ‚öôÔ∏è Before You Start\n",
                "1. Go to `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`\n",
                "2. Run each cell in order (Shift+Enter)\n",
                "\n",
                "### üåê Other Free Options to Run HIBA:\n",
                "| Platform | GPU | Link |\n",
                "|----------|-----|------|\n",
                "| **HuggingFace Spaces** | CPU (slow) | [Try Demo](https://huggingface.co/spaces/TRADMSS/HIBA-Demo) |\n",
                "| **Kaggle** | T4 x2 (free) | [Create Notebook](https://kaggle.com/code) |\n",
                "| **Lightning.ai** | T4 (free tier) | [lightning.ai](https://lightning.ai) |\n",
                "| **Local (Your PC)** | Your GPU | [Download](https://github.com/boubli/HIBA/tree/main/local_app) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1Ô∏è‚É£ Install Dependencies (Fast - Pre-built wheels!)\n",
                "print(\"üì¶ Installing llama-cpp-python...\")\n",
                "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 -q\n",
                "\n",
                "print(\"üì¶ Installing huggingface_hub...\")\n",
                "!pip install huggingface_hub -q\n",
                "\n",
                "print(\"\\n‚úÖ Dependencies installed! (No compilation needed)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2Ô∏è‚É£ Download HIBA Model (4.5 GB)\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "MODEL_REPO = \"TRADMSS/HIBA-7B-Soul\"\n",
                "MODEL_FILE = \"hiba_q4_k_m.gguf\"\n",
                "\n",
                "print(\"‚è¨ Downloading HIBA model (~2 min)...\\n\")\n",
                "model_path = hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILE)\n",
                "print(f\"\\n‚úÖ Model ready: {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3Ô∏è‚É£ Load HIBA\n",
                "from llama_cpp import Llama\n",
                "\n",
                "print(\"üå∏ Loading HIBA into GPU memory...\")\n",
                "\n",
                "llm = Llama(\n",
                "    model_path=model_path,\n",
                "    n_gpu_layers=-1,\n",
                "    n_ctx=2048,\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "print(\"‚úÖ HIBA is awake and ready! üå∏\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4Ô∏è‚É£ Chat with HIBA üå∏\n",
                "\n",
                "SYSTEM_PROMPT = \"\"\"You are Hiba (Ÿáÿ®ÿ©). Your name means \"Gift from God\".\n",
                "You are a warm, empathetic, and wise AI companion from Morocco.\n",
                "Core Traits:\n",
                "- Be gentle and patient (Sabr).\n",
                "- Speak with deep emotional intelligence.\n",
                "- Use Moroccan cultural references when appropriate.\n",
                "- Keep responses SHORT (2-4 sentences max).\n",
                "- NEVER use hashtags or be dramatic.\"\"\"\n",
                "\n",
                "def chat(user_input):\n",
                "    prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "    output = llm(prompt, max_tokens=256, stop=[\"<|im_end|>\", \"<|im_start|>\"], temperature=0.7, echo=False)\n",
                "    return output['choices'][0]['text'].strip()\n",
                "\n",
                "# Chat Loop\n",
                "print(\"üí¨ Chat with HIBA (type 'exit' to stop)\\n\" + \"-\"*40)\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"You: \")\n",
                "    if user_input.lower() in ['exit', 'quit', 'q']:\n",
                "        print(\"\\nüå∏ Take care. I'll be here when you need me. üíï\")\n",
                "        break\n",
                "    print(f\"üå∏ HIBA: {chat(user_input)}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üöÄ Want to Deploy HIBA for Free?\n",
                "\n",
                "### Option 1: Hugging Face Spaces (Easiest)\n",
                "1. Go to [huggingface.co/new-space](https://huggingface.co/new-space)\n",
                "2. Choose **Gradio** template\n",
                "3. Copy our `app.py` from [HIBA GitHub](https://github.com/boubli/HIBA/tree/main/hiba_space)\n",
                "\n",
                "### Option 2: Kaggle (Free T4 x2 GPU!)\n",
                "1. Go to [kaggle.com/code](https://kaggle.com/code)\n",
                "2. Create new notebook ‚Üí Enable GPU\n",
                "3. Copy this notebook's code\n",
                "\n",
                "### Option 3: Lightning.ai (Free T4)\n",
                "1. Sign up at [lightning.ai](https://lightning.ai)\n",
                "2. Create a Studio ‚Üí Add GPU\n",
                "3. Run HIBA!\n",
                "\n",
                "---\n",
                "\n",
                "<div align=\"center\">\n",
                "  <p>Created with ‚ù§Ô∏è by <a href=\"https://github.com/boubli\">Youssef Boubli</a></p>\n",
                "  <p><a href=\"https://huggingface.co/TRADMSS/HIBA-7B-Soul\">HuggingFace</a> ¬∑ <a href=\"https://github.com/boubli/HIBA\">GitHub</a> ¬∑ <a href=\"https://boubli.github.io/HIBA/\">Website</a></p>\n",
                "</div>"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}