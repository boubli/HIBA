{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå∏ HIBA-7B-Soul: Official Quickstart (Google Colab)\n",
                "\n",
                "<div align=\"center\">\n",
                "  <img src=\"https://github.com/boubli/HIBA/raw/main/docs/assets/logo.png\" width=\"150\" alt=\"HIBA Logo\"/>\n",
                "  <h1>HIBA: The Therapeutic AI (Free Tier)</h1>\n",
                "  <p>Run the specialist emotional support model completely free on Google Colab's T4 GPU.</p>\n",
                "</div>\n",
                "\n",
                "### ‚ö†Ô∏è System Prompt\n",
                "HIBA works best when you remind her who she is. This notebook includes the optimized system prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies\n",
                "# Installing llama-cpp-python with CUDA support for GPU acceleration\n",
                "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
                "!pip install huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Download Model (Q4_K_M)\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "model_name = \"TRADMSS/HIBA-7B-Soul\"\n",
                "model_file = \"hiba_q4_k_m.gguf\"\n",
                "\n",
                "print(\"‚è¨ Downloading HIBA (4.7 GB)... this may take 1-2 minutes...\")\n",
                "model_path = hf_hub_download(repo_id=model_name, filename=model_file)\n",
                "print(f\"‚úÖ Model downloaded to: {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Load HIBA-7B-Soul\n",
                "from llama_cpp import Llama\n",
                "\n",
                "llm = Llama(\n",
                "    model_path=model_path,\n",
                "    n_gpu_layers=-1, # Offload all layers to GPU\n",
                "    n_ctx=2048,\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "print(\"üå∏ HIBA is awake and ready to listen.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Chat with HIBA üå∏\n",
                "\n",
                "SYSTEM_PROMPT = \"\"\"You are Hiba (Ÿáÿ®ÿ©). Your name means \"Gift from God\".\n",
                "You are a warm, empathetic, and wise AI companion from Morocco.\n",
                "Core Traits:\n",
                "- Be gentle and patient (Sabr).\n",
                "- Speak with deep emotional intelligence.\n",
                "- Use Moroccan cultural references when appropriate.\n",
                "- Keep responses SHORT (2-4 sentences max).\n",
                "- NEVER use hashtags or be dramatic.\"\"\"\n",
                "\n",
                "def chat(user_input):\n",
                "    prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "    \n",
                "    output = llm(\n",
                "        prompt,\n",
                "        max_tokens=256,\n",
                "        stop=[\"<|im_end|>\", \"<|im_start|>\"],\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        echo=False\n",
                "    )\n",
                "    return output['choices'][0]['text'].strip()\n",
                "\n",
                "# Interactive Loop\n",
                "print(\"Type 'exit' to end the conversation.\\n\")\n",
                "while True:\n",
                "    user_input = input(\"You: \")\n",
                "    if user_input.lower() in ['exit', 'quit']:\n",
                "        break\n",
                "    \n",
                "    response = chat(user_input)\n",
                "    print(f\"üå∏ HIBA: {response}\\n\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}