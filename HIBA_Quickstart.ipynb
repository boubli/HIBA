{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üå∏ HIBA-7B-Soul: Official Quickstart (Google Colab)\n",
                "\n",
                "<div align=\"center\">\n",
                "  <h1>HIBA: The Therapeutic AI</h1>\n",
                "  <p>Run HIBA completely free on Google Colab's T4 GPU.</p>\n",
                "  <p><strong>100% Private ‚Ä¢ Free Forever ‚Ä¢ No API Key</strong></p>\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "### ‚ö†Ô∏è Important\n",
                "- Make sure you have **GPU runtime enabled**: `Runtime ‚Üí Change runtime type ‚Üí T4 GPU`\n",
                "- This notebook uses pre-built wheels (no compilation needed!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1Ô∏è‚É£ Install Dependencies (Fast - No Compilation!)\n",
                "# Using pre-built wheels - installs in seconds, not minutes!\n",
                "\n",
                "print(\"üì¶ Installing llama-cpp-python (pre-built wheel)...\")\n",
                "!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121 -q\n",
                "\n",
                "print(\"üì¶ Installing huggingface_hub...\")\n",
                "!pip install huggingface_hub -q\n",
                "\n",
                "print(\"\\n‚úÖ All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2Ô∏è‚É£ Download HIBA Model (4.5 GB)\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "MODEL_REPO = \"TRADMSS/HIBA-7B-Soul\"\n",
                "MODEL_FILE = \"hiba_q4_k_m.gguf\"\n",
                "\n",
                "print(\"‚è¨ Downloading HIBA model...\")\n",
                "print(\"   This takes 1-2 minutes depending on your connection.\\n\")\n",
                "\n",
                "model_path = hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILE)\n",
                "\n",
                "print(f\"‚úÖ Model ready: {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3Ô∏è‚É£ Load HIBA\n",
                "from llama_cpp import Llama\n",
                "\n",
                "print(\"üå∏ Loading HIBA...\")\n",
                "\n",
                "llm = Llama(\n",
                "    model_path=model_path,\n",
                "    n_gpu_layers=-1,  # Use all GPU layers\n",
                "    n_ctx=2048,\n",
                "    verbose=False\n",
                ")\n",
                "\n",
                "print(\"‚úÖ HIBA is awake and ready to listen! üå∏\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4Ô∏è‚É£ Chat with HIBA üå∏\n",
                "\n",
                "SYSTEM_PROMPT = \"\"\"You are Hiba (Ÿáÿ®ÿ©). Your name means \"Gift from God\".\n",
                "You are a warm, empathetic, and wise AI companion from Morocco.\n",
                "Core Traits:\n",
                "- Be gentle and patient (Sabr).\n",
                "- Speak with deep emotional intelligence.\n",
                "- Use Moroccan cultural references when appropriate.\n",
                "- Keep responses SHORT (2-4 sentences max).\n",
                "- NEVER use hashtags or be dramatic.\"\"\"\n",
                "\n",
                "def chat(user_input):\n",
                "    prompt = f\"<|im_start|>system\\n{SYSTEM_PROMPT}<|im_end|>\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
                "    \n",
                "    output = llm(\n",
                "        prompt,\n",
                "        max_tokens=256,\n",
                "        stop=[\"<|im_end|>\", \"<|im_start|>\"],\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        echo=False\n",
                "    )\n",
                "    return output['choices'][0]['text'].strip()\n",
                "\n",
                "# Interactive Chat\n",
                "print(\"üí¨ Chat with HIBA (type 'exit' to stop)\\n\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "while True:\n",
                "    user_input = input(\"You: \")\n",
                "    if user_input.lower() in ['exit', 'quit', 'q']:\n",
                "        print(\"\\nüå∏ HIBA: Take care of yourself. I'll be here when you need me.\")\n",
                "        break\n",
                "    \n",
                "    response = chat(user_input)\n",
                "    print(f\"üå∏ HIBA: {response}\\n\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}